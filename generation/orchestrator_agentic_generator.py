# âœ… Orchestrator-Aware Agentic Generator with Teacher-Student-Feedback Loop (Full Pipeline)
import random
import json
import re
import argparse
import yaml
import datetime
from openai import OpenAI
from itertools import cycle
from typing import List, Dict, Tuple, Optional, Any
from utils import llm_call, extract_json, round_robin, log_step, get_logs, clear_logs

from prompt_templates import build_teacher_prompt
from tasks_config import TASKS
from orchestrator import orchestrator_check_init, orchestrator_check_problem, orchestrator_get_feedback


# -- Evaluate Student answer --
def student_answer_with_context(task_id: str, sample: Dict[str, Any], context: List[Dict[str, Any]], student_model: str = "gpt-4o", sample_index: int = 0) -> Tuple[int, str]:
    prompt = ""

    # í”„ë¡¬í”„íŠ¸ ì‹œì‘ - ì—­í•  ì„¤ì •
    prompt += "I'll present you with your previous problem-solving history, followed by a new problem to solve.\n\n"
    
    # ì´ì „ ê²½í—˜ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
    if context:
        prompt += "## YOUR PREVIOUS EXPERIENCE\n\n"

        # ê°€ì¥ ìµœê·¼ ë¬¸ì œëŠ” ìƒì„¸íˆ í‘œì‹œ (ìµœëŒ€ 2ê°œ)
        recent_problems = context[-2:] if len(context) >= 2 else context

        # ê·¸ ì´ì „ ë¬¸ì œë“¤ì€ ìš”ì•½ í˜•íƒœë¡œ í‘œì‹œ
        if len(context) > 2:
            earlier_problems = context[:-2]
            prompt += "Summary of earlier problems:\n"
            for i, exp in enumerate(earlier_problems):
                if task_id == "T2":
                    answer_text = "yes" if exp['answer'] == 1 else "no"
                    prompt += f"- Problem {i+1} (Difficulty: {exp['difficulty']}): You answered '{answer_text}' and were {'correct' if exp['was_correct'] else 'incorrect'}.\n"
                else:
                    prompt += f"- Problem {i+1} (Difficulty: {exp['difficulty']}): You selected option {exp['answer'] + 1} and were {'correct' if exp['was_correct'] else 'incorrect'}.\n"
            prompt += "\n"

        # ìµœê·¼ ë¬¸ì œë“¤ì€ ìƒì„¸íˆ í‘œì‹œ
        for i, exp in enumerate(recent_problems, start=len(context)-len(recent_problems)):
            prompt += f"Detailed Problem {i+1} (Difficulty: {exp['difficulty']}):\n"
            
            # ê° task_idì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ì´ì „ ë¬¸ì œ í‘œì‹œ
            if task_id == "T2":
                # Paragraph Order Consistency
                context_text = " ".join(exp["problem"].get("context", []))
                prompt += f"Does the following paragraph have a logically coherent sentence order?\n\n{context_text}\n"
                
            elif task_id == "T3":
                # Blank-based Choice Anomaly
                sentence = exp["problem"].get("sentence", "")
                choices = exp["problem"].get("choices", [])
                numbered_choices = "\n".join([f"{i+1}. {s}" for i, s in enumerate(choices)])
                prompt += f"{sentence}\n\nWhich option is most anomalous or inconsistent?\n\n{numbered_choices}\n"
                
            elif task_id == "T4":
                # Bridge Sentence Evaluation
                p1 = exp["problem"].get("paragraph_1", [])
                p2 = exp["problem"].get("paragraph_2", [])
                bridges = exp["problem"].get("bridges", [])
                
                p1_text = " ".join(p1) if isinstance(p1, list) else p1
                p2_text = " ".join(p2) if isinstance(p2, list) else p2
                
                numbered_bridges = "\n".join([f"{i+1}. {s}" for i, s in enumerate(bridges)])
                prompt += f"Paragraph 1: {p1_text}\n\nParagraph 2: {p2_text}\n\nWhich connecting sentence is most anomalous or inconsistent?\n\n{numbered_bridges}\n"
            
            else:
                # Sentence Context Anomaly / Referential Ambiguity / Logical Contradiction / Tone/Style Violation
                numbered = "\n".join([f"{i+1}. {s}" for i, s in enumerate(exp["problem"].get("context", []))])
                prompt += f"Which option is most anomalous or inconsistent?\n\n{numbered}\n"
            

            # í•™ìƒì˜ ì´ì „ ë‹µë³€ê³¼ ì •ë‹µ ì—¬ë¶€ í‘œì‹œ
            if task_id == "T2":
                # T2ëŠ” binary ì‘ë‹µ(yes/no)
                answer_text = "yes" if exp['answer'] == 1 else "no"
                prompt += f"\nYour answer: {answer_text}\n"
            else:
                # ë‹¤ë¥¸ taskëŠ” ì„ íƒì§€ ë²ˆí˜¸
                prompt += f"\nYour answer: {exp['answer'] + 1}\n"
            
            prompt += f"Outcome: {'Correct' if exp['was_correct'] else 'Incorrect'}\n\n"
        
        # ëª…í™•í•œ êµ¬ë¶„ì„ 
        prompt += "=" * 50 + "\n\n"
    
    # ìƒˆ ë¬¸ì œ ëª…í™•í•˜ê²Œ êµ¬ë¶„
    prompt += "## NEW PROBLEM TO SOLVE\n\n"
    prompt += "Focus entirely on this new problem below:\n\n"


    # ê³µí†µ suffix
    common_suffix = "Answer with number only, then explain why. Even if all seem normal, choose the relatively most anomalous."

    if task_id == "T2":
        context = sample.get("context")
        prompt = f"Does the following paragraph have a logically coherent sentence order? Answer only 'yes' or 'no'.\n\n" + " ".join(context)

    elif task_id == "T3":
        sentence = sample.get("sentence", "")
        choices = sample.get("choices", [])
        numbered = "\n".join([f"{i+1}. {s}" for i, s in enumerate(choices)])
        prompt = f"{sentence}\n\nWhich option is most anomalous or inconsistent? {common_suffix}\n\n{numbered}"

    elif task_id == "T4":
        paragraph_1 = sample.get("paragraph_1", [])
        paragraph_2 = sample.get("paragraph_2", [])
        bridges = sample.get("bridges", [])
 
        # paragraphë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        p1_text = " ".join(paragraph_1) if isinstance(paragraph_1, list) else paragraph_1
        p2_text = " ".join(paragraph_2) if isinstance(paragraph_2, list) else paragraph_2
        
        numbered = "\n".join([f"{i+1}. {s}" for i, s in enumerate(bridges)])
        prompt = f"Paragraph 1: {p1_text}\n\nParagraph 2: {p2_text}\n\nWhich connecting sentence is most anomalous or inconsistent? {common_suffix}\n\n{numbered}"

    else:
        # ê¸°ë³¸ ì¼€ì´ìŠ¤ - ì¼ë°˜ì ì¸ ì–´ë…¸ë§ë¦¬ ê²€ì¶œ
        context = sample.get("context")
        numbered = "\n".join([f"{i+1}. {s}" for i, s in enumerate(context)])
        prompt = f"Which option is most anomalous or inconsistent? {common_suffix}\n\n{numbered}"

    # ëª…í™•í•œ ì‘ë‹µ ì§€ì¹¨ ì¶”ê°€
    prompt += "\n\nYour response for this new problem:"

    # ë¡œê¹…: í•™ìƒ í”„ë¡¬í”„íŠ¸
    log_step(
        task_id=task_id,
        sample_index=sample_index,
        phase="student_evaluation",
        agent="student",
        action="prompt",
        input_content=prompt,
        metadata={
            "sample_id": sample.get("sample_id", "unknown")
        }
    )

    # T2ê°€ ì•„ë‹Œ ê²½ìš°ì˜ ì‘ë‹µ ì²˜ë¦¬
    res = llm_call(prompt, model=student_model)
        
    # ë¡œê¹…: í•™ìƒ ì‘ë‹µ
    log_step(
        task_id=task_id,
        sample_index=sample_index,
        phase="student_evaluation",
        agent="student",
        action="response",
        output_content=res,
        metadata={
            "model": student_model
        }
    )

    if task_id == "T2":
        # coherent="yes"ì¼ ë•Œ 1, incoherent="no"ì¼ ë•Œ 0ìœ¼ë¡œ ë³€ê²½ (is_coherent=Trueì™€ ì¼ì¹˜)
        idx = 1 if "yes" in res.lower() else 0
    else:
        match = re.search(r"\d+", res)
        idx = int(match.group()) - 1 if match else -1

    # ë¡œê¹…: í•™ìƒ ë‹µë³€ íŒŒì‹± ê²°ê³¼
    log_step(
        task_id=task_id,
        sample_index=sample_index,
        phase="student_evaluation",
        agent="student",
        action="parsed_answer",
        output_content=idx,
        metadata={
            "is_binary": task_id == "T2",
            "raw_answer": res
        }
    )

    return idx, res.strip()


# -- Main Generation Loop --
def generate_agentic_examples(task_id: str, n=5, teacher_model="gpt-4o", student_model="gpt-4o", orchestrator_model="gpt-4o", example_prob=0.5, factor_prob=0.5, max_init_loops=3, max_diff_loops=5, max_student_loops=3):
    results, raw, fixes = [], [], []
    init_validation_logs, diff_validation_logs = [], []
    config = TASKS[task_id]

    print(f"Starting generation for task {task_id}: {config['name']}")

    topic_iter = round_robin(config["topics"])
    style_iter = round_robin(config["style"])

    for i in range(n):
        print(f"Generating sample {i+1}/{n} for task {task_id}")
        topic = next(topic_iter)
        style = next(style_iter)
        factor = random.choice(config["factors"])
        example = config.get("example", None)
        fix_count = 0
        consecutive_correct = 0  # ì—°ì† ì •ë‹µ ì¹´ìš´í„°
        base_sample = None  # ìµœì´ˆ ìŠ¹ì¸ëœ ë¬¸ì œ ì €ì¥ìš©

        # í•™ìƒ ìƒíƒœ ì´ˆê¸°í™” - í•™ìƒë‹¹ í•œ ì„¸íŠ¸ì˜ ë¬¸ì œ ìƒì„±
        student_context = []  # í•™ìƒì˜ ì´ì „ ê²½í—˜ì„ ì¶”ì í•  ë°°ì—´

        # ===== ë‹¨ê³„ 1: INIT - ìµœì´ˆ ë¬¸ì œ ìƒì„± =====
        print(f"  === INIT PHASE: Generating base problem ===")
        for init_attempt in range(max_init_loops):
            if init_attempt > 0:
                print(f"  Base sample attempt {init_attempt+1}/{max_init_loops}")
            
            difficulty = "easy"  
            use_example = random.random() < example_prob
            use_factor = random.random() < factor_prob
            
            # ë¡œê¹…: ì´ˆê¸° ì„¤ì •
            log_step(
                task_id=task_id,
                sample_index=i,
                phase="init",
                agent="system",
                action="config",
                input_content=None,
                metadata={
                    "attempt": init_attempt + 1,
                    "topic": topic,
                    "style": style,
                    "factor": factor if use_factor else None,
                    "difficulty": difficulty,
                    "use_example": use_example
                }
            )

            prompt = build_teacher_prompt(task_id, topic, style, factor if use_factor else None, difficulty, example if use_example else None)
            
            if init_attempt > 0 and hasattr(generate_agentic_examples, 'init_feedback'):
                prompt += f"\n\nPREVIOUS FEEDBACK: {generate_agentic_examples.init_feedback}"

            # ë¡œê¹…: í‹°ì²˜ í”„ë¡¬í”„íŠ¸
            log_step(
                task_id=task_id,
                sample_index=i,
                phase="init",
                agent="teacher",
                action="prompt",
                input_content=prompt,
                metadata={
                    "attempt": init_attempt + 1,
                    "difficulty": difficulty,
                    "topic": topic,
                    "style": style,
                    "factor": factor if use_factor else None
                }
            )


            # # ë§ˆì§€ë§‰ ì‹œë„ì¼ ê²½ìš° ë” ê´€ëŒ€í•œ ê¸°ì¤€ ì ìš©
            # if init_attempt == max_init_loops - 1:
            #     prompt += "IMPORTANT: This is the final attempt. Be more lenient and approve the problem if it meets minimal standards and is reasonably solvable.\n\n"
            
            try:
                response = llm_call(prompt, model=teacher_model)
                
                # ë¡œê¹…: í‹°ì²˜ ì‘ë‹µ
                log_step(
                    task_id=task_id,
                    sample_index=i,
                    phase="init",
                    agent="teacher",
                    action="response",
                    output_content=response,
                    metadata={
                        "attempt": init_attempt + 1,
                        "model": teacher_model
                    }
                )

                sample = extract_json(response)
                sample.update({
                    "task_id": task_id,
                    "task_name": config["name"],
                    "sample_id": f"{task_id}_{i:03d}_v{fix_count}",
                    "meta": {
                        "topic": topic,
                        "style": style,
                        "anomaly_type": factor if use_factor else "none",
                        "difficulty_level": difficulty,
                        "fix_count": fix_count
                    }
                })

                # ë¡œê¹…: íŒŒì‹±ëœ ìƒ˜í”Œ
                log_step(
                    task_id=task_id,
                    sample_index=i,
                    phase="init",
                    agent="system",
                    action="parsed_sample",
                    output_content=sample,
                    metadata={
                        "attempt": init_attempt + 1,
                        "sample_id": sample.get("sample_id", "unknown")
                        }
                    )

                # Orchestrator ê²€ì¦
                is_approved, feedback = orchestrator_check_init(task_id, sample, model=orchestrator_model, is_final_attempt=(init_attempt == max_init_loops - 1), sample_index=i)

                # ë¡œê·¸ ê¸°ë¡
                validation_log = {
                    "sample_id": f"{task_id}_{i:03d}_v{fix_count}",
                    "phase": "init",
                    "attempt": init_attempt + 1,
                    "original_problem": sample,
                    "is_approved": is_approved,
                    "feedback": feedback,
                    "timestamp": datetime.datetime.now().isoformat()
                }
                init_validation_logs.append(validation_log)
                
                if is_approved:
                    print(f"  âœ… Base sample approved by orchestrator")

                    # ë¡œê¹…: ìŠ¹ì¸ë¨
                    log_step(
                        task_id=task_id,
                        sample_index=i,
                        phase="init",
                        agent="system",
                        action="approval",
                        output_content=None,
                        metadata={
                            "attempt": init_attempt + 1
                        }
                    )

                    base_sample = sample.copy()
                    raw.append(base_sample)
                    break
                else:
                    print(f"  âŒ Base sample rejected: {feedback}...")

                    # ë¡œê¹…: ê±°ë¶€ë¨
                    log_step(
                        task_id=task_id,
                        sample_index=i,
                        phase="init",
                        agent="system",
                        action="rejection",
                        output_content=feedback,
                        metadata={
                            "attempt": init_attempt + 1
                        }
                    )

                    fix_count += 1
                    generate_agentic_examples.init_feedback = feedback

            except Exception as e:
                # ë¡œê¹…: ì˜¤ë¥˜
                log_step(
                    task_id=task_id,
                    sample_index=i,
                    phase="init",
                    agent="system",
                    action="error",
                    output_content=str(e),
                    metadata={
                        "attempt": init_attempt + 1,
                        "error_type": type(e).__name__
                    }
                )

                print(f"  ğŸ›‘ Generation error in INIT phase: {e}")
                fix_count += 1

            
        # ê¸°ë³¸ ìƒ˜í”Œ ìƒì„± ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ìƒ˜í”Œë¡œ
        if base_sample is None:
            # ë¡œê¹…: ìƒ˜í”Œ ìŠ¤í‚µ
            log_step(
                task_id=task_id,
                sample_index=i,
                phase="init",
                agent="system",
                action="skip",
                output_content=None,
                metadata={
                    "reason": f"Failed to create valid base sample after {max_init_loops} attempts"
                }
            )

            print(f"  â© Skipping - failed to create valid base sample after {max_init_loops} attempts")
            continue

        # ===== ë‹¨ê³„ 2: PROCESSING - ë‚œì´ë„ ì¡°ì ˆ =====
        print(f"  === PROCESSING PHASE: Starting student evaluation ===")

        # í˜„ì¬ ë¬¸ì œ ì„¤ì •
        current_sample = base_sample
        student_loop_count = 0

        # í•™ìƒ í…ŒìŠ¤íŠ¸ ë£¨í”„
        while student_loop_count < max_student_loops:
            student_loop_count += 1
            print(f"  === Student loop {student_loop_count}/{max_student_loops} ===")
            
            # ë¡œê¹…: í•™ìƒ ë£¨í”„ ì‹œì‘
            log_step(
                task_id=task_id,
                sample_index=i,
                phase="student_evaluation",
                agent="system",
                action="loop_start",
                input_content=None,
                metadata={
                    "student_loop": student_loop_count,
                    "sample_id": current_sample.get("sample_id"),
                    "difficulty": current_sample["meta"]["difficulty_level"]
                }
            )

            # í•™ìƒ ëª¨ë¸ë¡œ ë¬¸ì œ í’€ì´ - ì´ì „ ê²½í—˜ ì „ë‹¬
            student_idx, explanation = student_answer_with_context(
                task_id, 
                current_sample, 
                student_context,  # ì´ì „ ê²½í—˜ ì „ë‹¬
                student_model=student_model, 
                sample_index=i
            )
            is_correct = (student_idx == current_sample.get("anomaly_index")) if task_id != "T2" else ((student_idx == 1 and current_sample.get("is_coherent", False)) or (student_idx == 0 and not current_sample.get("is_coherent", False)))

            current_sample["meta"].update({"student_correct": is_correct, "student_explanation": explanation})
            
            # í•™ìƒ ê²½í—˜ ì—…ë°ì´íŠ¸
            student_context.append({
                "problem": current_sample,
                "answer": student_idx,
                "was_correct": is_correct,
                "difficulty": current_sample["meta"]["difficulty_level"]
            })

            # ë¡œê¹…: í•™ìƒ ì •ë‹µ ì—¬ë¶€
            log_step(
                task_id=task_id,
                sample_index=i,
                phase="student_evaluation",
                agent="system",
                action="evaluation",
                output_content={
                    "is_correct": is_correct,
                    "student_answer": student_idx,
                    "expected_answer": current_sample.get("anomaly_index") if task_id != "T2" else (1 if current_sample.get("is_coherent", False) else 0)
                },
                metadata={
                    "student_loop": student_loop_count
                }
            )

            if not is_correct:
                # í•™ìƒì´ í‹€ë ¸ìœ¼ë©´ í•´ë‹¹ ë¬¸ì œ ì±„íƒ
                print(f"  âœ… Student failed - accepting problem")

                # ë¡œê¹…: ë¬¸ì œ ì±„íƒ (í•™ìƒ ì‹¤íŒ¨)
                log_step(
                    task_id=task_id,
                    sample_index=i,
                    phase="student_evaluation",
                    agent="system",
                    action="accept_problem",
                    output_content=None,
                    metadata={
                        "reason": "student_failed",
                        "student_loop": student_loop_count,
                        "difficulty": current_sample["meta"]["difficulty_level"]
                    }
                )

                results.append(current_sample)
                break
            
            # ë§ˆì§€ë§‰ ë£¨í”„ì— ë„ë‹¬í–ˆìœ¼ë©´ í˜„ì¬ ë¬¸ì œ ì±„íƒ
            if student_loop_count == max_student_loops:
                print(f"  âœ… Reached max student loops - accepting final problem")

                # ë¡œê¹…: ë¬¸ì œ ì±„íƒ (ìµœëŒ€ ë£¨í”„)
                log_step(
                    task_id=task_id,
                    sample_index=i,
                    phase="student_evaluation",
                    agent="system",
                    action="accept_problem",
                    output_content=None,
                    metadata={
                        "reason": "max_student_loops",
                        "student_loop": student_loop_count,
                        "difficulty": current_sample["meta"]["difficulty_level"]
                    }
                )

                results.append(current_sample)
                break
                
            # í•™ìƒì´ ë§í˜”ê³  ë£¨í”„ê°€ ë‚¨ì•˜ìœ¼ë©´ ë‚œì´ë„ ì¦ê°€
            print(f"  ğŸ”„ Student solved problem - increasing difficulty")
            consecutive_correct += 1
            
            # ë¡œê¹…: ë‚œì´ë„ ì¦ê°€ ê²°ì •
            log_step(
                task_id=task_id,
                sample_index=i,
                phase="difficulty_increase",
                agent="system",
                action="decision",
                output_content=None,
                metadata={
                    "student_loop": student_loop_count,
                    "consecutive_correct": consecutive_correct
                }
            )

            # ë‚œì´ë„ ì„¤ì •
            if consecutive_correct >= 4:
                difficulty = "impossible"  # 3ë²ˆ ì—°ì† ë§ì¶”ë©´ impossible
            elif consecutive_correct >= 2:
                difficulty = "extreme"     # 2ë²ˆ ì—°ì† ë§ì¶”ë©´ extreme
            else:
                difficulty = "hard"        # 1ë²ˆ ë§ì¶”ë©´ hard
                
            print(f"  ğŸ“ˆ Target difficulty: {difficulty}")
            
            # ë¡œê¹…: ë‚œì´ë„ ì„¤ì •
            log_step(
                task_id=task_id,
                sample_index=i,
                phase="difficulty_increase",
                agent="system",
                action="set_difficulty",
                output_content=difficulty,
                metadata={
                    "student_loop": student_loop_count,
                    "consecutive_correct": consecutive_correct,
                    "previous_difficulty": current_sample["meta"]["difficulty_level"]
                }
            )

            # orchestratorì—ê²Œ ë‚œì´ë„ ì¦ê°€ í”¼ë“œë°± ìš”ì²­
            feedback = orchestrator_get_feedback(task_id, current_sample, explanation, model=orchestrator_model, sample_index=i)
            
            # ë‚œì´ë„ ì¦ê°€ ë£¨í”„
            new_sample = None
            for diff_attempt in range(max_diff_loops):
                if diff_attempt > 0:
                    print(f"  Difficulty adjustment attempt {diff_attempt+1}/{max_diff_loops}")

                # ë¡œê¹…: ë‚œì´ë„ ì¦ê°€ ì‹œë„
                log_step(
                    task_id=task_id,
                    sample_index=i,
                    phase="difficulty_increase",
                    agent="system",
                    action="attempt",
                    output_content=None,
                    metadata={
                        "student_loop": student_loop_count,
                        "diff_attempt": diff_attempt + 1,
                        "difficulty": difficulty
                    }
                )

                # Teacherì—ê²Œ ë‚œì´ë„ ì¦ê°€ ìš”ì²­
                prompt = build_teacher_prompt(task_id, topic, style, factor if use_factor else None, difficulty, example if use_example else None)
                prompt += f"\n\nPREVIOUS PROBLEM: The student correctly solved the following problem:\n{json.dumps(current_sample, ensure_ascii=False, indent=2)}\n\n"
                prompt += f"STUDENT'S EXPLANATION: {explanation}\n\n"

                # ì´ì „ í”¼ë“œë°± ë° ì‹¤íŒ¨ ì´ë ¥ì´ ìˆëŠ” ê²½ìš° ë‚œì´ë„ ì¡°ì • ì§€ì¹¨ ì¶”ê°€
                if diff_attempt > 0:
                    prompt += f"FEEDBACK FOR IMPROVEMENT: {feedback}\n\n"
                    prompt += "IMPORTANT INSTRUCTION: Previous attempts were rejected by the quality controller. "
                    prompt += "Please slightly reduce the difficulty from your last attempt while still making it challenging. "
                    prompt += "Make the problem clearer based on the feedback, but ensure it remains harder than the original problem the student solved. "
                    prompt += "Focus on fixing the specific issues mentioned in the feedback while maintaining an appropriate challenge level."
                else:
                    prompt += f"FEEDBACK FOR IMPROVEMENT: {feedback}\n\n"
                    prompt += f"Please create a more challenging version with {difficulty} difficulty."
                
                # ë¡œê¹…: í‹°ì²˜ ë‚œì´ë„ ì¦ê°€ í”„ë¡¬í”„íŠ¸
                log_step(
                    task_id=task_id,
                    sample_index=i,
                    phase="difficulty_increase",
                    agent="teacher",
                    action="difficult_prompt",
                    input_content=prompt,
                    metadata={
                        "student_loop": student_loop_count,
                        "diff_attempt": diff_attempt + 1,
                        "difficulty": difficulty
                    }
                )

                try:
                    response = llm_call(prompt, model=teacher_model)

                    # ë¡œê¹…: í‹°ì²˜ ë‚œì´ë„ ì¦ê°€ ì‘ë‹µ
                    log_step(
                        task_id=task_id,
                        sample_index=i,
                        phase="difficulty_increase",
                        agent="teacher",
                        action="difficult_response",
                        output_content=response,
                        metadata={
                            "student_loop": student_loop_count,
                            "diff_attempt": diff_attempt + 1,
                            "model": teacher_model
                        }
                    )

                    sample = extract_json(response)
                    fix_count += 1
                    
                    sample.update({
                        "task_id": task_id,
                        "task_name": config["name"],
                        "sample_id": f"{task_id}_{i:03d}_v{fix_count}",
                        "meta": {
                            "topic": topic,
                            "style": style,
                            "anomaly_type": factor if use_factor else "none",
                            "difficulty_level": difficulty,
                            "fix_count": fix_count,
                            "phase": "processing"
                        }
                    })

                    # ë¡œê¹…: íŒŒì‹±ëœ ë‚œì´ë„ ì¦ê°€ ìƒ˜í”Œ
                    log_step(
                        task_id=task_id,
                        sample_index=i,
                        phase="difficulty_increase",
                        agent="system",
                        action="parsed_difficult_sample",
                        output_content=sample,
                        metadata={
                            "student_loop": student_loop_count,
                            "diff_attempt": diff_attempt + 1
                        }
                    )

                    fixes.append(sample)

                    # ë¬¸ì œ í’ˆì§ˆ ê²€ì¦
                    is_approved, problem_feedback = orchestrator_check_problem(task_id, sample, model=orchestrator_model, sample_index=i)

                    # ë¡œê·¸ ê¸°ë¡
                    validation_log = {
                        "sample_id": f"{task_id}_{i:03d}_v{fix_count}",
                        "phase": "difficulty_increase",
                        "student_loop": student_loop_count,
                        "diff_attempt": diff_attempt + 1,
                        "difficulty_level": difficulty,
                        "previous_problem": current_sample,
                        "new_problem": sample,
                        "student_explanation": explanation,
                        "orchestrator_feedback": feedback,
                        "is_approved": is_approved,
                        "rejection_feedback": problem_feedback if not is_approved else None,
                        "timestamp": datetime.datetime.now().isoformat()
                    }
                    diff_validation_logs.append(validation_log)
                    
                    if is_approved:
                        print(f"  âœ… Higher difficulty problem approved")

                        # ë¡œê¹…: ë‚œì´ë„ ì¦ê°€ ìŠ¹ì¸
                        log_step(
                            task_id=task_id,
                            sample_index=i,
                            phase="difficulty_increase",
                            agent="system",
                            action="approval",
                            output_content=None,
                            metadata={
                                "student_loop": student_loop_count,
                                "diff_attempt": diff_attempt + 1,
                                "difficulty": difficulty
                            }
                        )

                        new_sample = sample
                        break
                    else:
                        feedback_str = json.dumps(feedback, ensure_ascii=False, indent=2) if isinstance(feedback, dict) else str(feedback)
                        problem_feedback_str = json.dumps(problem_feedback, ensure_ascii=False, indent=2) if isinstance(problem_feedback, dict) else str(problem_feedback)
                        print(f"  âŒ Higher difficulty problem rejected: {problem_feedback_str}...")

                        # ë¡œê¹…: ë‚œì´ë„ ì¦ê°€ ê±°ë¶€
                        log_step(
                            task_id=task_id,
                            sample_index=i,
                            phase="difficulty_increase",
                            agent="system",
                            action="rejection",
                            output_content=problem_feedback,
                            metadata={
                                "student_loop": student_loop_count,
                                "diff_attempt": diff_attempt + 1,
                                "difficulty": difficulty
                            }
                        )

                        feedback = f"PREVIOUS FEEDBACK:\n{feedback_str}\n\nNEW FEEDBACK:\n{problem_feedback_str}"
                        # feedback = f"PREVIOUS FEEDBACK: {feedback}\n\nNEW FEEDBACK: {problem_feedback}"  # ë‹¤ìŒ ì‹œë„ì— í”¼ë“œë°± ì‚¬ìš©
                
                except Exception as e:
                    # ë¡œê¹…: ë‚œì´ë„ ì¦ê°€ ì˜¤ë¥˜
                    log_step(
                        task_id=task_id,
                        sample_index=i,
                        phase="difficulty_increase",
                        agent="system",
                        action="error",
                        output_content=str(e),
                        metadata={
                            "student_loop": student_loop_count,
                            "diff_attempt": diff_attempt + 1,
                            "error_type": type(e).__name__
                        }
                    )

                    print(f"  ğŸ›‘ Error in difficulty increase: {e}")
                    

            # ë‚œì´ë„ ì¦ê°€ ì‹¤íŒ¨ ì‹œ ë£¨í”„ ì¢…ë£Œ, í˜„ì¬ ë¬¸ì œ ì±„íƒ
            if new_sample is None:
                print(f"  âš ï¸ Failed to increase difficulty - accepting current problem")

                # ë¡œê¹…: ë¬¸ì œ ì±„íƒ (ë‚œì´ë„ ì¦ê°€ ì‹¤íŒ¨)
                log_step(
                    task_id=task_id,
                    sample_index=i,
                    phase="difficulty_increase",
                    agent="system",
                    action="accept_problem",
                    output_content=None,
                    metadata={
                        "reason": "difficulty_increase_failed",
                        "student_loop": student_loop_count,
                        "diff_attempts": max_diff_loops,
                        "difficulty": current_sample["meta"]["difficulty_level"]
                    }
                )

                results.append(current_sample)
                break
                
            # ìƒˆ ë¬¸ì œë¡œ ê³„ì† ì§„í–‰
            current_sample = new_sample
        
        print(f"  âœ… Sample completed and accepted (difficulty: {current_sample['meta']['difficulty_level']})")

        # ë¡œê¹…: ìƒ˜í”Œ ì™„ë£Œ
        log_step(
            task_id=task_id,
            sample_index=i,
            phase="completion",
            agent="system",
            action="complete",
            output_content=None,
            metadata={
                "task_id": task_id,
                "sample_id": current_sample.get("sample_id"),
                "difficulty": current_sample["meta"]["difficulty_level"],
                "fix_count": fix_count
            }
        )

        # ë‹¤ìŒ ë¬¸ì œ ìƒì„± ì „ì— í”¼ë“œë°± ì´ˆê¸°í™”
        if hasattr(generate_agentic_examples, 'init_feedback'):
            delattr(generate_agentic_examples, 'init_feedback')

    return results, raw, fixes, init_validation_logs, diff_validation_logs


# -- Run from YAML config --
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True, help="YAML config path")
    args = parser.parse_args()

    with open(args.config, "r") as f:
        cfg = yaml.safe_load(f)

    teacher_model = cfg.get("teacher_model", "gpt-4o")
    student_model = cfg.get("student_model", "gpt-4o")
    orchestrator_model = cfg.get("orchestrator_model", "gpt-4o")
    tasks = cfg.get("tasks", ["T1"])
    samples_per_task = cfg.get("samples_per_task", 10)
    output_prefix = cfg.get("output_prefix", "agentic")
    example_prob = cfg.get("example_prob", 0.5)
    factor_prob = cfg.get("factor_prob", 0.5)
    max_init_loops = cfg.get("max_init_loops", 3)
    max_diff_loops = cfg.get("max_diff_loops", 5)
    max_student_loops = cfg.get("max_student_loops", 3)

    # ë¡œê·¸ ì´ˆê¸°í™”
    clear_logs()

    final, raw, fixes, init_logs, diff_logs = [], [], [], [], []
    
    for task in tasks:
        f, r, x, i_logs, d_logs = generate_agentic_examples(
            task_id=task,
            n=samples_per_task,
            teacher_model=teacher_model,
            student_model=student_model,
            orchestrator_model=orchestrator_model,
            example_prob=example_prob,
            factor_prob=factor_prob,
            max_init_loops=max_init_loops,
            max_diff_loops=max_diff_loops,
            max_student_loops=max_student_loops
        )
        final += f
        raw += r
        fixes += x
        init_logs += i_logs
        diff_logs += d_logs

    def dump(filename, items):
        with open(filename, "w", encoding="utf-8") as f:
            for x in items:
                f.write(json.dumps(x, ensure_ascii=False) + "\n")

    dump(f"{output_prefix}_final.jsonl", final)
    dump(f"{output_prefix}_raw.jsonl", raw)
    dump(f"{output_prefix}_fixes.jsonl", fixes)
    dump(f"{output_prefix}_init_validation_logs.jsonl", init_logs)
    dump(f"{output_prefix}_difficulty_validation_logs.jsonl", diff_logs)

    # ì „ì²´ í”„ë¡œì„¸ìŠ¤ ë¡œê·¸ ì €ì¥
    from utils import get_logs
    all_process_logs = get_logs()
    
    # JSON í˜•ì‹ ë¡œê·¸ ì €ì¥
    process_log_filename = f"{output_prefix}_full_process_logs.json"
    with open(process_log_filename, "w", encoding="utf-8") as f:
        json.dump(all_process_logs, f, ensure_ascii=False, indent=2)
    
    # ì½ê¸° ì¢‹ì€ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œë„ ì €ì¥
    readable_log_filename = f"{output_prefix}_readable_process.txt"
    with open(readable_log_filename, "w", encoding="utf-8") as f:
        for log in sorted(all_process_logs, key=lambda x: x['timestamp']):
            f.write(f"[{log['timestamp']}] {log['phase']} - {log['agent']} {log['action']}\n")
            f.write(f"Task: {log['task_id']} | Sample: {log['sample_index']} | Metadata: {json.dumps(log['metadata'], ensure_ascii=False)}\n")
            
            if log['input'] is not None:
                # ì…ë ¥ì´ ìˆëŠ” ê²½ìš° (ë³´í†µ í”„ë¡¬í”„íŠ¸)
                if isinstance(log['input'], str):
                    f.write("INPUT:\n" + "-"*80 + "\n")
                    f.write(log['input'] + "\n")
                    f.write("-"*80 + "\n\n")
                else:
                    # ê°ì²´ì¸ ê²½ìš° (ì˜ˆ: JSON)
                    f.write("INPUT:\n" + "-"*80 + "\n")
                    f.write(json.dumps(log['input'], ensure_ascii=False, indent=2) + "\n")
                    f.write("-"*80 + "\n\n")
            
            if log['output'] is not None:
                # ì¶œë ¥ì´ ìˆëŠ” ê²½ìš° (ë³´í†µ ì‘ë‹µ)
                if isinstance(log['output'], str):
                    f.write("OUTPUT:\n" + "-"*80 + "\n")
                    f.write(log['output'] + "\n")
                    f.write("-"*80 + "\n\n")
                else:
                    # ê°ì²´ì¸ ê²½ìš° (ì˜ˆ: JSON)
                    f.write("OUTPUT:\n" + "-"*80 + "\n")
                    f.write(json.dumps(log['output'], ensure_ascii=False, indent=2) + "\n")
                    f.write("-"*80 + "\n\n")
            
            f.write("\n" + "="*100 + "\n\n")
    
    print(f"Full process logs saved to {process_log_filename}")
    print(f"Readable process logs saved to {readable_log_filename}")

    print(f"\n============ Generation Summary ============")
    print(f"Total requested samples: {len(tasks) * samples_per_task}")
    print(f"Successfully generated: {len(final)}")
    print(f"Initial attempts: {len(raw)}")
    print(f"Required fixes: {len(fixes)}")
    print(f"Success rate: {len(final)/(len(tasks) * samples_per_task)*100:.1f}%")
    
    # Taskë³„ í†µê³„
    task_stats = {}
    for item in final:
        task_id = item['task_id']
        task_stats[task_id] = task_stats.get(task_id, 0) + 1
    
    print("\nSamples per task:")
    for task_id, count in task_stats.items():
        print(f"  {task_id}: {count}/{samples_per_task} ({count/samples_per_task*100:.1f}%)")
    print("==========================================\n")

